{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) #X1 and X2\n",
    "Y = np.array([[0, 0, 0, 1]]).T #Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function for the single unit in the o/p unit\n",
    "def sig(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d(sig(z))/d(z)\n",
    "def derivative_sig(z):\n",
    "    return sig(z) * (1 - sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.20007817],\n",
       "        [-0.16827016]]),\n",
       " array([0.11647475]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing weights\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "bias = 2 * np.random.random(1) - 1\n",
    "lr = 0.1 #learning rate for updating weights using gradient descent\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.47433843],\n",
       "        [5.47433846]]),\n",
       " array([-8.30422242]),\n",
       " array([[2.47408472e-04],\n",
       "        [5.57305043e-02],\n",
       "        [5.57305029e-02],\n",
       "        [9.33668371e-01]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    #Forward propagation\n",
    "    output0 = X\n",
    "    output = sig(np.dot(output0, weights) + bias)\n",
    "\n",
    "    #gradient descent for weight optimization\n",
    "    first_term = output - Y #d(E)/d(Oj) = y_pred - y_actual\n",
    "    input_for_last_layer = np.dot(output0, weights) + bias #i/p_j\n",
    "    second_term = derivative_sig(input_for_last_layer) #d(Oj)/d(i/p_j) = sig(z)(1 - sig(z))\n",
    "    first_two = first_term * second_term\n",
    "    #Now, we have our first two terms of gradient descent ready. And, as expected their dimension is 4X1, as we said in the notes.\n",
    "    #Use this to verify: first_two.shape\n",
    "    #Now, let's compute the third term: d(i/p_j)/d(w_ij), and then we need to  multipy it with the first two terms and do summation for each training data point in order to get our gradient descent.\n",
    "    #In our code, to do that we just need to:\n",
    "    #For w11: multiply each x1 value with its corresponding element of first_two\n",
    "    #For w21: multiply each x2 value with its corresponding element of first_two\n",
    "    changes = np.array([[0.0], [0.0]]) # array in which we will store the summation (the sum values). Same dimension as that of weights i.e. 2X1 \n",
    "    #So, changes, will be our updated weights!\n",
    "    for i in range(2): #we have to go through values of 2 features x1 nad x2\n",
    "        for j in range(4): #we have 4 rows i.e. 4 values of each feature\n",
    "            #If we're dealing with w11, then changes[0][0], for w21 -> changes[1][0]; therefore, changes[i_th feature][0_th column]\n",
    "            changes[i][0] += first_two[j][0] * output0[j][i] \n",
    "    weights = weights - lr * changes\n",
    "    #We're done with weights updation, but, we still have to find a way to update the biases.\n",
    "    #It's exactly same, we just need to multiply  with 1 instead of output[j][i]. Also, as we just have a single bias, we don't iterate twice, we just iterate once over the 4 rows of i/p\n",
    "    bias_change = 0.0\n",
    "    for j in range(4):\n",
    "        bias_change += first_two[j][0] * 1\n",
    "    bias = bias - lr * bias_change\n",
    "    #Finally, we have successfully updated our bias as well as our weights\n",
    "\n",
    "    #We'll run this cell many times, say 10000, as we updating just once won't be enough to get the optimal weights\n",
    "output = sig(np.dot(X, weights) + bias) #calculating o/p final time\n",
    "weights, bias, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The third array shows the output. We expect it to be 0,0,0,1 but it is what you are seeing above. However, it does tell us\n",
    "#that we're going in the correct direction uptil now, as the last one is >0.5, the first three are <0.5.\n",
    "#But, we want the last one to be very close to 1 and the first three to be very close to 0.\n",
    "#So, first of all let's try and change the learning rate, make it 0.01\n",
    "\n",
    "#Yes, as we incresed the learning rate from 0.001 to 0.01, we got much better results.\n",
    "#Let's give another shot and make it 0.1\n",
    "#Yay! We're getting much better results now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus, we have successfully trained our first neural network having no hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.47567819],\n",
       "        [5.47567819]]),\n",
       " array([-8.30622607]),\n",
       " array([[2.46913370e-04],\n",
       "        [5.56955759e-02],\n",
       "        [5.56955762e-02],\n",
       "        [9.33710215e-01]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimizing above cell's code by replacing for loops for weight updation with vector operations, here, metrix multiplication\n",
    "#And replacing for loop for bias updation with np.sum\n",
    "for iter in range(10000):\n",
    "    #Forward propagation\n",
    "    output0 = X\n",
    "    output = sig(np.dot(output0, weights) + bias)\n",
    "\n",
    "    #gradient descent for weight optimization\n",
    "    first_term = output - Y #d(E)/d(Oj) = y_pred - y_actual\n",
    "    input_for_last_layer = np.dot(output0, weights) + bias #i/p_j\n",
    "    second_term = derivative_sig(input_for_last_layer) #d(Oj)/d(i/p_j) = sig(z)(1 - sig(z))\n",
    "    first_two = first_term * second_term\n",
    "    #Now, we have our first two terms of gradient descent ready. And, as expected their dimension is 4X1, as we said in the notes.\n",
    "    #Use this to verify: first_two.shape\n",
    "    #Now, let's compute the third term: d(i/p_j)/d(w_ij), and then we need to  multipy it with the first two terms and do summation for each training data point in order to get our gradient descent.\n",
    "    #In our code, to do that we just need to:\n",
    "    #For w11: multiply each x1 value with its corresponding element of first_two\n",
    "    #For w21: multiply each x2 value with its corresponding element of first_two\n",
    "    changes = np.dot(output0.T, first_two)\n",
    "    weights = weights - lr * changes\n",
    "    #We're done with weights updation, but, we still have to find a way to update the biases.\n",
    "    #It's exactly same, we just need to multiply  with 1 instead of output[j][i]. Also, as we just have a single bias, we don't iterate twice, we just iterate once over the 4 rows of i/p\n",
    "    bias_change = np.sum(first_two)\n",
    "    bias = bias - lr * bias_change\n",
    "    #Finally, we have successfully updated our bias as well as our weights\n",
    "\n",
    "    #We'll run this cell many times, say 10000, as we updating just once won't be enough to get the optimal weights\n",
    "output = sig(np.dot(X, weights) + bias) #calculating o/p final time\n",
    "weights, bias, output\n",
    "\n",
    "#Before running this cell run the weight initialization cell once again to avoid the neural network to be pre-trained due to the above un-optimized code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus, we have successfully trained our first neural network having no hidden layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
