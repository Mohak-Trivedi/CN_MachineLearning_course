{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation for a simple neural network which we will build from scratch, which has just an i/p layer containing a \n",
    "#bias and X1 and X2, and an o/p layer having a single unit which has  a sigmoid function. We will use the AND dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) #X1 and X2\n",
    "Y = np.array([0, 0, 0, 1]) #Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to convert our Y into a 2D array and then transpose it because, as it is 1D array it looks like:\n",
    "#[0, 0, 0, 1] whereas X is like:\n",
    "#[[0, 0],\n",
    "#[0, 1],\n",
    "#[1, 0],\n",
    "#[1, 1]]\n",
    "#So, Y should be like:\n",
    "#[[0],\n",
    "#[0],\n",
    "#[0],\n",
    "#[1]]\n",
    "Y = np.array([[0, 0, 0, 1]])\n",
    "#now Y looks like:\n",
    "#[[0, 0, 0, 1]]\n",
    "#Thus, we need to do transpose, to get desirable Y\n",
    "Y = np.array([[0, 0, 0, 1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 1]]),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y #perfect dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function for the single unit in the o/p unit\n",
    "def sig(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68618963],\n",
       "       [0.87205669]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's initialize our weights (random weights, we'll correct them using backpropagation later.)\n",
    "#We'll keep them a bit generic\n",
    "#We'll use the np.random.random() as it will generate random values for our weights in the dimension which we want(i.e. the\n",
    "#dimension that suits our dataset -> (2, 1) -> (2 rows, 1 column), thus, giving us two weights w1 and w2, for X1 and X2,\n",
    "#respecively).\n",
    "weights = np.random.random((2, 1))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.81515584],\n",
       "        [-0.35005123]]),\n",
       " array([-0.79287428]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#right now, the random weights are being genrated within the range of  0 to 1.\n",
    "#But, then we won't have any random weights.\n",
    "#So, to get them in a range of -1 to 1, we just multiply our current range with 2, so it becomes 0 to 2, and then we subrtract\n",
    "#1 from it, so the range moves 1 space back, and we get -1 to 1\n",
    "#Similarly, let's also get our bias. In this problem, we're having  only single  bias, but that won't always be the case.\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "bias = 2 * np.random.random(1) - 1\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus, we have all our required weights initialized.\n",
    "#Now's the time to write the code for forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31155184],\n",
       "       [0.24178364],\n",
       "       [0.50557016],\n",
       "       [0.41878339]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We know that the output of the i/p layer is the i/p itself, here X\n",
    "output0 = X\n",
    "\n",
    "#In our current e.g. we don't have any hidden layers, directly an o/p layer with a single unit which implements sigmoid function\n",
    "#In order to get z (= WX + B = w1x1 + w2x2 + b), we won't be multiplying each x to its corresponding weight, but we'll use\n",
    "#dot product of 'output0' and 'weights'.\n",
    "output = sig(np.dot(output0, weights) + bias)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus, we have successfully implemented forward propagation. But, this was a simple neural network with no hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
